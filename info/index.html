<!doctype html>

<html lang="en">
<head>
    <meta charset="utf-8">

    <title>math189r</title>
    <meta name="description" content="Harvey Mudd Mathematics of Big Data I">
    
    <link rel="stylesheet" href="../css/styles.css">
</head>

<body>

    <div class="header">
        <h2> <a href="/">math189r</a> </h2>
        <h3>
            <a href="#">Info</a>
        </h3>
    </div>

    <div class="body">
        <h3>Mathematics of Big Data I</h3>
        <p>
            Professor Weiqing Gu</br>
            Harvey Mudd College</br>
            Fall 2016
        </p>


        <div class="left-align">
        <h3> Meeting Times </h3>
        <p>
            M_ 06:30-09:15PM. SHAN B460 (Lecture)</br>
            Tr 06:30-08:00PM. SHAN B460 (Optional Section)</br>
        </p>

        <h3> Summary of Goals </h3>
        <ul>
            <li>
                Gain a comprehensive view of machine learning
                as an academic discipline and understand the
                mathematics behind it.
            </li>
            <li>
                Be able to read recent academic papers in
                the machine learning literature and apply
                those algorithms and concepts to real
                world problems.
            </li>
            <li>
                Become comfortable with industry and academia
                standard tools (such as AWS and GitHub) and be
                able to find and work with large, public datasets.
            </li>
        </ul>

        <h3> Description </h3>
        <p>
            This is a course in how to utilize data: infer,
            predict, coerce, and classify. We will cover a large
            breadth of material, spanning supervised and unsupervised
            learning, recommender systems, and
            Bayesian modelling, to a high level of mathematical
            rigor. Upon successful completion of the course, students
            should be fully equipped to enter industry as a data
            scientist, read active research in the field of Machine
            Learning, and approach huge (data and otherwise) problems
            seen in the real world.</br></br>

            Additionally, another goal of this course is to become
            comfortable using Amazon Web Services and GitHub as
            these tools are <i>extremely</i> prevalent in industry
            and academia when developing and deploying models. To
            that end, all code for homework and your final project
            will be hosted on GitHub.
        </p>

        <h3> Structure </h3>
        <p>
            There will be mandatory Monday
            lectures with readings to be completed before class
            (detailed below).</br></br>
            
            A section held each Thursday will either
            review prerequisite material, go over supplementary material,
            or investigate an interesting application of our coursework.
            They will either be taught by the instructor or a teaching
            assistant. For the review sections (the first two) attendance is recommended
            to anyone who (for the Linear Algebra review) doesn't know
            what any of {<a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky Decomposition</a>, SVD, inner product, outer product} are,
            or (for the Probability review) doesn't know what any of {Bayes' Rule,
            binomial distribution, Bernoulli distribution, multinomial distribution,
            Poisson distribution, Gaussian distribution, covariance} are. Notes
            will be posted prior to the meetings so just check those out before and
            see if you feel comfortable with the material. We expect around
            half of the students to be comfortable with more than 75% of the
            linear algebra and probability we will be using in the course. This
            is fine! Just come to the </br></br>

            For sections after the review sections (ie. Convex Optimization
            Overview, Sparsity; SVM Training), attendance is again not 
            required but highly (!) recommended. Sections are designed to give
            you inspiration and insight into your final project, and shed 
            light on the material in a new way.
        </p>

        <h3> Textbook </h3>
        <p>
            <a href="https://www.cs.ubc.ca/~murphyk/MLbook/">Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.</a>
        </p>

        <h3> Grading </h3>
        <ul>
            <li> 5%  Reading Summaries </li>
            <li> 35% Homework </li>
            <li> 20% Midterm </li>
            <li> 40% Final Project </li>
            <li> [Up to 5% Extra Credit] </li>
        </ul>

        <h3> Reading Summaries </h3>
        <p>
            All readings are compulsory, but some are more compulsory
            than others.</br></br>

            To encourage the goal of reading active research in
            the field, half-page reading summaries for all non-Murphy readings
            will be due at the beginning of class. They must be
            <b>legible</b>, and demonstrate that you have read the
            paper with a high degree of confidence. Credit will be
            given on a {0-10} scale for each summary. Your summaries
            should be written at a high level, and should focus on
            the main point of the readings (ie. avoid complicated
            math). As long as your summary is reasonable, you will
            be given full credit.
        </p>

        <h3> Homework (Every ~2 Weeks. Due Monday) </h3>
        <p>
            For coding: feel free to use any of {R, Julia, Python, Matlab}.
            If you want to use something not on that list, just ask us
            with a good reason and we'll probably say yes.
            </br></br>

            The homework is split approximately evenly between
            mathematical analysis and extension of our course material
            and application of algorithms to real world data.</br></br>
        </p>

        <h3> Midterm </h3>
        <p>
            The midterm will be 3 hour take-home exam covering all topics
            seen until October 10 (inclusive). You will receive the exam
            in class on Oct. 10 (the Monday before break) and have until
            Oct. 24 (the Monday after break) to complete it. More detailed
            instructions will be given on the exam, but you are to turn the exam
            into the designated box outside Prof. Gu's office immediately after
            completion, or, if she is not there, under the door. The hard
            cut-off for handing the exam in is at the beginning of class
            on Oct. 24.
        </p>

        <h2> Final Project </h2>
        <h4> Description </h4>
        <p>
            This is by far the largest component of the course.
            You will discover, explore, and attack a real world 
            problem of your choosing. There are 3 types of projects you
            can work on, shown below in order of increasing difficulty.
            <ol>
                <li>
                    Application of existing algorithm to a
                    new problem and potentially new data.
                </li>
                <li>
                    Algorithmic work. Extend an existing algorithm
                    or conceive a new one to solve some problem.
                    This inherently includes (1) because you will
                    need to test this new algorithm on data.
                </li>
                <li>
                    Theoretical work. Create a new convergence bound
                    on a learning algorithm. Show that at some limit
                    one learning algorithm becomes another. Etc.
                </li>
            </ol>
            These also have increasing risk. For example, you cannot
            turn in a paper saying you worked on a convergence bound
            for months with no results. Type (2) has medium risk because
            part of the process of creating a new algorithm is creating
            baselines to improve upon.</br></br>

            At <i>any</i> time during the course <i>please</i> feel free
            to come and discuss your problem and ask questions with the
            instructor or TAs.
        </p>
        <h4> Requirements </h4>
        <ul>
            <li>
                Maximum of 1 partner (we may concede to 2 partners
                in extreme scenarios eg. huge coding project). All
                partners must contribute equally.
            </li>
            <li>
                You must use at least one (1) dataset with at least
                one (1) million data points as a significant part of
                your project.
            </li>
            <li>
                Your submission must be submitted as a pdf in 
                <a href="https://nips.cc">NIPS</a> <a href="https://nips.cc/Conferences/2016/PaperInformation/AuthorSubmissionInstructions">format</a>. Note
                that this means you must use LaTeX with their
                <a href="https://nips.cc/Conferences/2016/PaperInformation/StyleFiles">style file</a>. 
                (NIPS, Neural Information Processing Systems,
                is one of the major machine learning conferences).
            </li>
            <li>
                All code used in the production of your final
                report should be clean
                <a href="https://drivendata.github.io/cookiecutter-data-science/">(suggested format)</a>
                and placed into a public GitHub repository under one of
                your partner's accounts. Place a footnote to this
                URL somewhere in your final pdf. This is not required but
                it is recommended to place your code under
                <a href="http://choosealicense.com/">some open-source license</a>
                such as <a href="http://choosealicense.com/licenses/mit/">MIT</a>.
            </li>
        </ul>
        <h4> Due Dates </h4>
        <p>
            Only one copy of each item need be turned in per group.
        </p>
        <ul>
            <li>
                <b>Sept. 26</b> - Final Project Proposal. Typed (LaTeX)
                one (1) page maximum explaining your problem, what data
                sets you are likely to use (you must find some candidates),
                who your partners are, and what methods (of those you know of)
                you think you might use. Note that this is not 100% final
                but it should be within some epsilon of your final project.
            </li>
            <li> 
                <b>Oct. 24</b> - Midterm Progress Report. Typed (LaTeX);
                three (3) page maximum detailing your (significant) progress
                towards your goal, which algorithms you have used, any
                necessary insight (mathematical or otherwise) you have
                conjured, etc. If you have not made significant progress
                you must still show everything you have tried (which is
                expected to be a lot). The progress report is due the same
                time as your take home midterm.
            </li>
            <li>
                <b>Dec. 5</b> - Final Project Submission. Must conform to
                the requirements above (NOTE: NIPS papers have a hard cap
                at 9 pages and a cap at 8. Read the formatting requirements!)
            </li>
            <li>
                <b>Week of Nov. 28</b> or <b>Week of Dec. 5</b> - Final
                Project Presentation. 10 minutes long. The instructor and
                class will choose up to 10 extraordinary presentations
                to be shown on the last day of presentation. We will allocate
                3 days for the class to present during these two weeks
                according with everyone's schedules.
            </li>
        </ul>

        <h3> Disabilities </h3>
        <p>
            Students who need disability-related accommodations are
            encouraged to discuss this with the instructor as soon as
            possible.
        </p>

        <h3> Teaching Assistants (TAs) </h3>
        <table style="text-align: center;">
            <thead>
                <tr>
                    <th>Name</th>
                    <th>Email (@hmc.edu)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <th>Conner DiPaolo (head grader/tutor)</th>
                    <th>cdipaolo</th>
                </tr>
                <tr>
                    <th>Paul David (CGU Graduate Student)</th>
                    <th>paul.david (@cgu.edu)</th>
                </tr>
                <tr>
                    <th>Kathryn Dover</th>
                    <th>kdover</th>
                </tr>
                <tr>
                    <th>Zoe Tucker</th>
                    <th>ztucker</th>
                </tr>
                <tr>
                    <th>Ricky Pan</th>
                    <th>rpan</th>
                </tr>
                <tr>
                    <th>Natchanon Suaysom</th>
                    <th>nsuaysom</th>
                </tr>
                <tr>
                    <th>Mek Jenrungrot</th>
                    <th>mjenrungrot</th>
                </tr>
                <tr>
                    <th>Herrick Fang</th>
                    <th>hfang</th>
                </tr>
                <tr>
                    <th>Bo Zhang</th>
                    <th>bzhang</th>
                </tr>
            </tbody>
        </table>
        </div>

</body>
</html>
