<!DOCTYPE HTML>
<!--
	Solarize by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Syllabus</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
	</head>
	<body>

		<!-- Header Wrapper -->
			<div class="wrapper style1">

			<!-- Header -->
				<div id="header">
					<div class="container">

						<!-- Logo -->
							<h1><a href="#" id="logo">Big Data</a></h1>

						<!-- Nav -->
							<nav id="nav">
								<ul>
									<li class="active"><a href="index.html">Overview</a></li>
									<li><a href="syllabus.html">Syllabus</a></li>
									<li><a href="schedule.html">Course Schedule</a></li>
									<li><a href="project.html">Final Project</a></li>
									<li><a href="resources.html">Resources</a></li>
								</ul>
							</nav>

					</div>
				</div>
			</div>



		<!-- Main -->
			<div id="main" class="wrapper style6">

				<!-- Content -->
				<div id="content" class="container">
					<section>
						<header class="major">
							<h2 align="center">Mathematics of Big Data</h2>
							<span class="byline" align="center">
								Professor Weiqing Gu <br>
								Summer 2017
							</span>
						</header>

						<p>
							<b>Readings</b> should be done <i>before</i> class. Reading
							summaries are due for all non-Murphy readings at the start of class.
						</p>


						<table>
						    <thead>
						        <tr>
						            <th> <b>Date</b> </th>
						            <th> <b>Topics</b> </th>
						            <th> <b>Homework</b> </th>
						        </tr>
						    </thead>
						    <tbody>

						        <!-- Supervised Learning -->
						        <tr>
						            <td><b>Supervised Learning</b></br> May 15</td>
						            <td>
						                Introduction to Big Data <br>
														Linear Regression <br>
														Normal Equations and Optimization Techniques<br>
														Solving the Normal Equations efficiently
														(Cholesky Decomposition)<br>
						                Various forms of Linear regression
						            </td>
						            <td>
						                <b>Read:</b> <br>
														Murphy 1.{all} (DEFINITELY READ BEFORE FIRST CLASS)</br>
														Murphy, 7.{1,...,5}
						            </td>
						        </tr>

						        <tr>
						            <td>May 17</td>
						            <td>
						                Classification<br>
														K-Nearest Neighbors<br>
						                Logistic Regression<br>
														Exponential Family and Generalized Linear Models<br>
														Logistic Regression as a GLM
						            <td>
													<b>Read:</b><br>
													Murphy, 8.{1,2,3,5} \ 8.{3.4,3.5}, 9.{1,2.2,2.4,3}
													<br><br>
													<b>Due:</b> <br>
													Homework 1
												</td>
						        </tr>

						        <tr>
						            <td>May 22</td>
						            <td>
						                Generalized Linear Models continued<br>
						                Poisson Regression<br>
														Softmax Regression<br>
						                Covariance matrix<br>
														Multivariate Gaussian Distribution<br>
														Marginalized Gaussian and the Schur Complement<br>
						                Regularization continued<br>
														Big Data as a Regularizer
						            </td>
						            <td>
					                <b>Read:</b> <br>
													Murphy 9.7, 4.{1,2,3,4,5,6} (important background)
													<br><br>
					                <b>Due:</b> <br>
													Homework 2
												</td>
						        </tr>

						        <tr>
						            <td>May 24</td>
						            <td>
						                Dimensionality Reduction <br>
														Spectral Decomposition<br>
														Singular Value Decomposition<br>
						                Principal Component Analysis<br>
						                Generative Learning Algorithms<br>
						                Gaussian Discriminant Analysis
						            </td>
						            <td>
														<b>Due:</b> <br>
														Final Project Proposal<br>
														Homework 1 (last third)
						            </td>
						        </tr>

						        <tr>
						           	<td>May 29</td>
						            <td>
						                Naive Bayes<br>
														L1 Regularization and Sparsity<br>
						                Lasso<br>
														Support Vector Machines<br>
														Kernels
						            </td>
						            <td>
														<b>Read:</b> <br>
														Murphy 14.{1,2,3,4} \ 14.{4.4}</br>
														<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf">
															MapReduce: Simplified Data Processing on Large Clusters
														</a>
														</br><br>
														<b>Due:</b> <br>
														Homework 3
												</td>
						        </tr>

						        <!-- Unsupervised Learning -->
						        <tr>
						            <td><b> Unsupervised Learning </b></br> May 31</td>
						            <td>
						                Introduction to Unsupervised Learning<br>
						                Clustering<br>
														K-Means<br>
														Mixture of Gaussians<br>
						                Expectation-Maximization (EM) Algorithm
						            </td>
						            <td>

														<b>Read:</b><br>
														Murphy 11.{1,2,3,4} \ 11.{4.6,4.9}</br>
														<a href="http://www.ee.oulu.fi/research/imag/courses/Vedaldi/ShalevSiSr07.pdf">
															Pegasos: Primal Estimated sub-GrAdient SOlver for SVM
														</a></br>
														<a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">
															Random Features for Large-Scale Kernel Machines</a>
														</br><br>
														<b>Due:</b> <br>
														Homework 4
						            </td>
						        </tr>

						        <tr>
						            <td>June 5</td>
						            <td>
						                Principal Component Analysis (PCA) Review<br>
						                Kernel PCA<br>
														One Class Support Vector Machines
						            </td>
						            <td>
													<b>Read:</b> <br>
													Murphy 12.2.{0,1,2,3} 14.4.4</br>
					               	<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.675.575&rep=rep1&type=pdf">
														Support Vector Method for Novelty Detection
													</a>
													<br></br>
					                <b>Due:</b><br>
													Midterm <br>
					                Final Project Progress Report<br>
						            </td>
						        </tr>

						        <tr>
						            <td><b> Learning Theory </b></br>June 7</td>
						            <td>
						                Learning Theory<br>
														VC Dimension<br>
														Bias/Variance Trade-off<br>
														Union and Chernoff/Hoeffding Bounds.
						            </td>
						            <td>
													<b>Read:</b> <br>
													<a href="http://papers.nips.cc/paper/4337-large-scale-sparse-principal-component-analysis-with-application-to-text-data.pdf">
														Large-Scale Sparse Principal Component Analysis with Application to Text Data
													</a></br>

													<a href="http://projecteuclid.org/download/pdf_1/euclid.aos/1176346060">
														On the Convergence Properties of the EM Algorithm
													</a>

													<br><br>
													<b>Due:</b> <br>
													Homework5
												</td>
						        </tr>

						        <!-- Recommender Systems -->
						        <tr>
						            <td><b> Recommender Systems </b></br>June 12</td>
						            <td>
						                Introduction to Recommender Systems<br>
														Collaborative
						                Filtering<br>
														Non-Negative Matrix Factorization<br>
						                Using Non-Negative Matrix Factorization for Topic
						                Modelling
						            </td>
						            <td>

						                <b>Read:</b><br>
														 Murphy 27.6.2</br>
						                <a href="http://sifter.org/~simon/journal/20061211.html">
															Netflix Update: Try This at Home</a>

														<br><br>
						                <b>Due:</b> <br>
														Homework5
						            </td>
						        </tr>

						        <!-- Graph Methods -->
						        <tr>
						            <td><b>Graph Methods</b></br>June 14</td>
						            <td>
						                Graphs<br>
														Graph representations as data<br>
														The Laplacian
						                and usage of Spectral (Eigenvalue-Eigenvector) information
						                <br>
						                Directed Graphical Models (Bayesian Networks)<br>
														Conditional
						                Independence<br>
														Naive Bayes
						                as a Graphical Model<br>
														Plate Notation
						            </td>
						            <td>
														<b>Read:</b> <br>
														Murphy 10.{1,2,3,4,5,6}
														<br><br>
														<b>Due:</b> <br>
														Homework 6
						            </td>
						        </tr>

						        <!-- Bayesian Learning -->
						        <tr>
						            <td><b>Bayesian Learning</b></br>June 19</td>
						            <td>
						                Recap of Bayesian Reasoning<br>
						                Bayesian Linear Regression (which we've already seen)<br>
						                Bayesian Logistic Regression<br>
														Intractable Integrals and
						                Motivation for Approximate Methods
						            </td>
						            <td>
						                <b>Read:</b><br>
														Murphy 5.{1,2,3.0,3.2} 7.6, 8.4</br>
						                <a href="https://hips.seas.harvard.edu/files/adams-changepoint-tr-2007.pdf">
															Bayesian Online Changepoint Detection</a>
														<br><br>
						                <b>Due:</b><br>
														Homework 7
												</td>
						        </tr>

						        <tr>
						            <td>June 21</td>
						            <td>
						                Monte-Carlo Methods<br>
														Rejection Sampling<br>
						                Importance Sampling<br>
														Intro to Markov-Chain
						                Monte-Carlo<br>
						                Gibbs Sampling<br>
														The Metropolis-Hastings
						                Algorithm
						            </td>
						            <td>

						                <b>Read:</b><br>
														 Murphy 23.{1,2,3,4} \ 23.4.3,<br>
						                 24.{1,2.(1,2,3,4), 3,4} \ 24.{3.7}
														<br><br>
						                <b>Optional Read:</b><br>
														Murphy 24.{5,6}
														<br><br>

						                <b>Due:</b> <br>
														Homework
						            </td>
						        </tr>


						        <tr>
						            <td><a href="">Dec.  5</a></td>
						            <td>
						                Latent Dirichlet Allocation<br>
						                Nonparametric Models<br>
														K-Nearest-Neighbors as
						                a Nonparametric Model<br>
														Gaussian Processes<br>
						                Dirichlet Processes and the infinite
														mixture of Gaussians
						            </td>
						            <td>
						                <b>Read:</b> <br>
														Murphy 15.{1,2,3,4}, 25.2, 27.{1,2,3}</br>
						                <a href="http://jmlr.org/proceedings/papers/v28/wilson13.pdf">
															Gaussian Process Kernels for Pattern Discovery
															and Extrapolation</a>

														<br><br>
						                <b>Due:</b> <br>
														Final Project
						            </td>
						        </tr>
						    </tbody>
						</table>

					</section>
				</div>
			</div>


			<!-- Copyright -->
				<div id="copyright">
					@Copyright 2017 Mathematics of Big Data Summer 2017 All Rights Reserved
				</div>

		</div>

	</body>
</html>
